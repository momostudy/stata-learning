{smcl}
{* 19aug2007}{...}
{hline}
{hi:help ivreg2}{right:(SJ7-4: st0030_3; SJ5-4: st0030_2;}
{right:SJ4-2: st0030_1; SJ3-1: st0030)}
{hline}

{title:Title}

{p2colset 5 15 17 2}{...}
{p2col:{hi:ivreg2} {hline 2}}Extended instrumental variables/2SLS, GMM and AC/HAC, LIML, and k-class regression{p_end}
{p2colreset}{...}


{title:Syntax}

{p 4 4 2}Full syntax

{p 8 14 2}{cmd:ivreg2} {it:depvar} [{it:varlist1}]
{cmd:(}{it:varlist2}{cmd:=}{it:varlist_iv}{cmd:)}
{ifin}
{weight}
{bind:[{cmd:,} {cmd:gmm} {cmd:gmm2s}}
{cmd:bw(}{it:#}|{cmd:auto}{cmd:)}
{cmd:kernel(}{it:string}{cmd:)}
{cmd:liml}
{cmd:fuller(}{it:#}{cmd:)}
{cmdab:k:class(}{it:#}{cmd:)}
{cmd:coviv}
{cmd:cue}
{cmd:cueinit}{cmd:(}{it:matrix}{cmd:)} 
{cmdab:cueopt:ions}{cmd:(}{it:string}{cmd:)} 
{cmd:b0}{cmd:(}{it:matrix}{cmd:)} 
{cmdab:r:obust}
{cmdab:cl:uster}{cmd:(}{it:varname}{cmd:)}
{cmd:orthog(}{it:varlist_ex}{cmd:)}
{cmdab:endog:test(}{it:varlist_en}{cmd:)}
{cmdab:red:undant(}{it:varlist_ex}{cmd:)}
{cmd:partial(}{it:varlist}{cmd:)}
{cmdab:sm:all}
{cmdab:noc:onstant} {cmdab:h:ascons}
{cmd:smatrix}{cmd:(}{it:matrix}{cmd:)} 
{cmd:wmatrix}{cmd:(}{it:matrix}{cmd:)} 
{cmd:first} {cmd:ffirst} {cmd:savefirst} {cmdab:savefp:refix}{cmd:(}{it:prefix}{cmd:)} 
{cmd:rf} {cmd:saverf} {cmdab:saverfp:refix}{cmd:(}{it:prefix}{cmd:)} 
{cmd:nocollin} {cmd:noid}
{cmdab:l:evel}{cmd:(}{it:#}{cmd:)}
{cmdab:nohe:ader}
{cmdab:nofo:oter}
{cmdab:ef:orm}{cmd:(}{it:string}{cmd:)} 
{cmdab:dep:name}{cmd:(}{it:varname}{cmd:)}
{bind:{cmd:plus}]}


{p 4 4 2}Replay syntax

{p 8 14 2}{cmd:ivreg2}
{bind:[{cmd:,} {cmd:first}}
{cmd:ffirst} {cmd:rf} 
{cmdab:l:evel}{cmd:(}{it:#}{cmd:)}
{cmdab:nohe:ader}
{cmdab:nofo:oter}
{cmdab:ef:orm}{cmd:(}{it:string}{cmd:)} 
{cmdab:dep:name}{cmd:(}{it:varname}{cmd:)}
{cmd:plus}]


{p 4 4 2}Version syntax

{p 8 14 2}{cmd:ivreg2}, {cmd:version}


{p 4 4 2}{cmd:ivreg2} may be used with time-series or panel data. 
You must {cmd:tsset} the data before using {cmd:ivreg2}; see
{manhelp tsset TS}.

{p 4 4 2}All {it:varlist}s may contain time-series operators; 
see {it:{help varlist}}.

{p 4 4 2}{cmd:by}, {cmd:rolling}, {cmd:statsby}, {cmd:xi},
{cmd:bootstrap}, and {cmd:jackknife} are allowed; see {help prefix}.

{p 4 4 2}{cmd:aweight}s, {cmd:fweight}s, {cmd:iweight}s, and {cmd:pweight}s
are allowed; see {help weight}.

{p 4 4 2}The syntax of {helpb predict} following {cmd:ivreg2} is

{p 8 16 2}{cmd:predict} [{it:type}] {it:newvarname} [{it:if}]
[{it:in}] [{cmd:,} {it:statistic}]

{p 4 4 2}where {it:statistic} is

{p 8 23 2}{cmd:xb}{space 11}fitted values; the default{p_end}
{p 8 23 2}{cmdab:r:esiduals}{space 4}residuals{p_end}
{p 8 23 2}{cmd:stdp}{space 9}standard error of the prediction{p_end}

{p 8 8 2}These statistics are available both in and out of sample;
type {cmd:predict} ... {cmd:if e(sample)} ... if wanted only for
the estimation sample.


{title:Contents}

{pstd}{help ivreg2##s_description:Description}{p_end}
{pstd}{help ivreg2##s_robust:Calculation of robust, cluster-robust, AC, and HAC standard errors}{p_end}
{pstd}{help ivreg2##s_gmm:GMM estimation}{p_end}
{pstd}{help ivreg2##s_liml:LIML, k-class, and CUE/GMM estimation}{p_end}
{pstd}{help ivreg2##s_sumopt:Summary of robust, HAC, AC, GMM, LIML, and CUE options}{p_end}
{pstd}{help ivreg2##s_overid:Testing overidentifying restrictions}{p_end}
{pstd}{help ivreg2##s_endog:Testing subsets of regressors and instruments for endogeneity}{p_end}
{pstd}{help ivreg2##s_relevance:Tests of under- and weak identification}{p_end}
{pstd}{help ivreg2##s_redundancy:Testing instrument redundancy}{p_end}
{pstd}{help ivreg2##s_first:First-stage regressions, identification, and weak-id-robust inference}{p_end}
{pstd}{help ivreg2##s_rf:Reduced form estimates}{p_end}
{pstd}{help ivreg2##s_partial:Partialling-out exogenous regressors}{p_end}
{pstd}{help ivreg2##s_ols:OLS and heteroskedastic OLS (HOLS) estimation}{p_end}
{pstd}{help ivreg2##s_collin:Collinearities}{p_end}
{pstd}{help ivreg2##s_speed:Speed options: nocollin and noid}{p_end}
{pstd}{help ivreg2##s_small:Small-sample corrections}{p_end}
{pstd}{help ivreg2##s_options:Options summary}{p_end}
{pstd}{help ivreg2##s_remarks:Remarks}{p_end}
{pstd}{help ivreg2##s_macros:Saved results}{p_end}
{pstd}{help ivreg2##s_examples:Examples}{p_end}
{pstd}{help ivreg2##s_refs:References}{p_end}
{pstd}{help ivreg2##s_acknow:Acknowledgments}{p_end}
{pstd}{help ivreg2##s_citation:Authors}{p_end}
{pstd}{help ivreg2##s_citation:Citation}{p_end}


{marker s_description}{title:Description}

{p 4 4 2}{cmd:ivreg2} implements a range of single-equation estimation methods
for the linear regression model: ordinary least squares (OLS), instrumental
variables (IV, also known as two-stage least squares, 2SLS), the generalized
method of moments (GMM), limited-information maximum likelihood (LIML), and
k-class estimators.  In the language of IV/GMM, {it:varlist1} are the exogenous
regressors or included instruments, {it:varlist_iv} are the exogenous variables
excluded from the regression or excluded instruments, and {it:varlist2} the
endogenous regressors that are being instrumented.

{p 4 4 2}{cmd:ivreg2} will also estimate linear regression models using
robust (heteroskedastic-consistent),
autocorrelation-consistent (AC),
heteroskedastic and autocorrelation-consistent (HAC)
and cluster-robust variance estimates.

{p 4 4 2}{cmd:ivreg2} provides extensions to Stata's official {cmd:ivregress}
and {cmd:newey}.
{cmd:ivreg2} supports the same command syntax as official {cmd:ivregress}
and (almost) all its options.
The main extensions available are as follows:
two-step feasible GMM estimation ({cmd:gmm2s} option)
and continuously updated GMM estimation ({cmd:cue} option);
LIML and k-class estimation;
automatic output of overidentification and underidentification test statistics;
C statistic test of exogeneity of subsets of instruments
({cmd:orthog()} option);
endogeneity tests of endogenous regressors
({cmd:endog()} option);
test of instrument redundancy
({cmd:redundant()} option);
kernel-based autocorrelation-consistent (AC),
and heteroskedastic and autocorrelation consistent (HAC) standard errors
and covariance estimation ({cmd:bw(}{it:#}{cmd:)} option),
with user-specified choice of kernel ({cmd:kernel()} option);
default reporting of large-sample statistics
(z and chi-squared rather than t and F);
{cmd:small} option to report small-sample statistics;
first-stage regressions reported with various tests and statistics for
identification and instrument relevance;
{cmd:ffirst} option to report only these identification statistics
and not the first-stage regression results themselves; 
{cmd:nofooter} option to suppress footer of regression output.
{cmd:ivreg2} can also be used for OLS estimation
using the same command syntax as official {cmd:regress} and {cmd:newey}.


    {marker s_robust}{title:Calculation of robust, cluster-robust, AC, and HAC standard errors}

{p 4 4 2}The standard errors reported by {cmd:ivreg2} can be made consistent
in the presence of a variety of violations of the assumption of independently
and identically distributed (i.i.d.) errors: {bind:(1) {cmd:robust}} causes
{cmd:ivreg2} to report standard errors that are robust to the presence of
arbitrary heteroskedasticity; {bind:(2) {cmd:cluster()}} standard errors are
robust to both arbitrary heteroskedasticity and arbitrary intragroup
correlation; {bind:(3) {cmd:bw(}{it:#}{cmd:)}} requests AC standard errors that
are robust to arbitrary autocorrelation; and {bind:(4) {cmd:bw(}{it:#}{cmd:)}}
combined with {cmd:robust} requests HAC standard errors that are robust to both
arbitrary heteroskedasticity and arbitrary autocorrelation.

{p 4 4 2}{cmd:ivreg2} allows a variety of options for kernel-based HAC and AC estimation.
The {cmd:bw(}{it:#}{cmd:)} option sets the bandwidth used in the estimation
and {cmd:kernel(}{it:string}{cmd:)} is the kernel used;
the default kernel is the Bartlett kernel,
also known in econometrics as Newey-West (see {helpb newey}).
When using the Bartlett, Parzen, or quadratic-spectral kernels, the automatic
bandwidth selection procedure of Newey and West (1994) can be chosen
by specifying {cmd:bw(auto)}.
{cmd:ivreg2} can also be used for kernel-based estimation
with panel data, i.e., a cross-section of time series.
Before using {cmd:ivreg2} for kernel-based estimation
of time-series or panel data,
the data must be {helpb tsset}.


    {marker s_gmm}{title:GMM estimation}

{p 4 4 2}When combined with the above options, the {cmd:gmm2s} option generates
efficient estimates of the coefficients as well as consistent
estimates of the standard errors.
The {cmd:gmm2s} option implements the two-step efficient
generalized method of moments (GMM) estimator.
The efficient GMM estimator minimizes the GMM criterion function
J=N*g'*W*g, where N is the sample size,
g are the orthogonality or moment conditions
(specifying that all the exogenous variables, or instruments,
in the equation are uncorrelated with the error term)
and W is a weighting matrix.
In two-step efficient GMM, the efficient or optimal weighting matrix
is the inverse of an estimate of the covariance matrix of orthogonality conditions.
The efficiency gains of this estimator relative to the
traditional IV/2SLS estimator derive from the use of the optimal
weighting matrix, the overidentifying restrictions of the model,
and the relaxation of the i.i.d. assumption.
For an exactly identified model,
the efficient GMM and traditional IV/2SLS estimators coincide,
and under the assumptions of conditional homoskedasticity and independence,
the efficient GMM estimator is the traditional IV/2SLS estimator.
For further details, see Hayashi (2000, 206-213 and 226-227).

{p 4 4 2}The {cmd:wmatrix()} option specifies a weighting matrix
rather than the optimal weighting matrix.
Estimation with the {cmd:wmatrix()} option yields a possibly inefficient GMM estimator.
{cmd:ivreg2} will use this inefficient estimator as the first-step GMM estimator
in two-step efficient GMM when combined with the {cmd:gmm2s} option;
otherwise, {cmd:ivreg2} reports the regression results
using this inefficient GMM estimator.

{p 4 4 2}The {cmd:smatrix()} option specifies the matrix S, the covariance
matrix of orthogonality conditions.  {cmd:ivreg2} will use this matrix in the
calculation of the variance-covariance matrix of the estimator, the J
statistic, and  if the {cmd:gmm2s} option is specified, the two-step efficient
GMM coefficients.  The {cmd:smatrix()} can be useful for guaranteeing a
positive test statistic in user-specified GMM-distance tests (see
{help ivreg2##s_endog:below}).  For further details, see Hayashi (2000,
220-224).


    {marker s_liml}{title:LIML, k-class, and CUE/GMM estimation}

{marker liml}{pstd} Maximum likelihood estimation of one equation of this
form (endogenous RHS variables and excluded instruments) is known as
LIML.  The overidentifying
restrictions test reported after LIML estimation is the Anderson-Rubin (1950)
overidentification statistic in a homoskedastic context.  LIML, OLS, and IV/2SLS
are examples of k-class estimators.  LIML is a k-class estimator with k=the
LIML eigenvalue lambda; 2SLS is a k-class estimator with k=1; and OLS is a
k-class estimator with k=0.  Estimators based on other values of k have been
proposed.  Fuller's modified LIML (available with the
{cmd:fuller(}{it:#}{cmd:)} option) sets k = lambda - alpha/(N-L), where lambda
is the LIML eigenvalue, L = number of instruments (L1 excluded and L2
included), and the Fuller parameter alpha is a user-specified positive
constant.  Nagar's bias-adjusted 2SLS estimator can be obtained with the
{cmd:kclass(}{it:#}{cmd:)} option by setting k = 1 + (L-K)/N, where L-K =
number of overidentifying restrictions, K = number of regressors (K1 endogenous
and K2=L2 exogenous) and N = the sample size.  For a discussion of LIML and
k-class estimators, see Davidson and MacKinnon (1993, 644-651).

{p 4 4 2} The GMM generalization of the LIML estimator
to the case of possibly heteroskedastic
and autocorrelated disturbances
is the continuously updated GMM estimator or CUE
of Hansen, Heaton, and Yaron (1996).
The CUE estimator directly maximizes the GMM objective function
J=N*g'*W(b_cue)*g, where W(b_cue) is an optimal weighting matrix
that depends on the estimated coefficients b_cue.
{cmd:cue} combined with {cmd:robust}, {cmd:cluster()}, and/or {cmd:bw()},
generates coefficient estimates that are efficient in the presence
of the corresponding deviations from homoskedasticity.
Specifying {cmd:cue} with no other options
is equivalent to the combination of the options {cmd:liml} and {cmd:coviv}.
The CUE estimator requires numerical optimization methods,
and the implementation here uses Stata's {cmd:ml} routine.
The starting values are either IV or two-step efficient GMM
coefficient estimates;
these can be overridden with the {cmd:cueinit()} option,
which takes the matrix of starting values b as its argument.
{cmd:cueoptions()} passes options to Stata's {cmd:ml}; see {manhelp ml R}.
Estimation with the {cmd:cue} option can be slow and problematic,
and it should be used with caution.
If the user wants to evaluate the CUE objective function at
an arbitrary user-defined coefficient vector instead of having {cmd:ivreg2}
find the coefficient vector that minimizes the objective function,
the {cmd:b0(}{it:matrix}{cmd:)} option can be used.
The value of the CUE objective function at {cmd:b0()}
is the Sargan or Hansen J statistic reported in the output.


    {marker s_sumopt}{title:Summary of robust, HAC, AC, GMM, LIML, and CUE options}

    {col 40}VCE option
    Estimator{col 60}{cmd:robust}, {cmd:cluster()},
     option{col 16}(none) {col 60}{cmd:bw()}, {cmd:kernel()}
    {hline}
    (none){col 16}IV/2SLS{col 60}IV/2SLS with
    {col 16}SEs consistent under homoskedasticity{col 60}robust SEs
    
    {cmd:liml}{col 16}LIML{col 60}LIML with
    {col 16}SEs consistent under homoskedasticity{col 60}robust SEs
    
    {cmd:gmm2s}{col 16}IV/2SLS{col 60}Two-step GMM with
    {col 16}SEs consistent under homoskedasticity{col 60}robust SEs
    
    {cmd:cue}{col 16}LIML{col 60}CUE/GMM with
    {col 16}SEs consistent under homoskedasticity{col 60}robust SEs
    
    {cmd:kclass()}{col 16}k-class estimator{col 60}k-class estimator 
    {col 16}SEs consistent under homoskedasticity{col 60}with robust SEs
    
    {cmd:wmatrix()}{col 16}Possibly inefficient GMM{col 60}Ineff GMM with
    {col 16}SEs consistent under homoskedasticity{col 60}robust SEs
    
    {cmd:gmm2s} + {col 16}Two-step GMM{col 60}two-step GMM with
    {cmd:wmatrix()}{col 16}with user-specified first step{col 60}robust SEs
    {col 16}SEs consistent under homoskedasticity
    {hline}
    
{p 4 4 2}With the {cmd:bw()} or {cmd:bw()} and {cmd:kernel()} VCE options, SEs
are autocorrelation-robust (AC).  Combining the {cmd:robust} option with
{cmd:bw()}, SEs are heteroskedasticity- and autocorrelation-robust (HAC). 
    
{p 4 4 2}For further details, see Hayashi (2000, 206-213 and 226-227) on GMM
estimation, Wooldridge (2002, 193) on cluster-robust GMM, and Hayashi
(2000, 406-410) or Cushing and McGarvey (1999) on kernel-based covariance
estimation.


    {marker s_overid}{marker overidtests}{title:Testing overidentifying restrictions}

{p 4 4 2}The Sargan-Hansen test is a test of overidentifying restrictions.
The joint null hypothesis is that the instruments are valid
instruments, i.e., uncorrelated with the error term, and that the excluded
instruments are correctly excluded from the estimated equation.  Under the
null, the test statistic is distributed as chi-squared in the number of (L-K)
overidentifying restrictions.  A rejection casts doubt on the validity of the
instruments.  For the efficient GMM estimator, the test statistic is Hansen's J
statistic, the minimized value of the GMM criterion function.  For the 2SLS
estimator, the test statistic is Sargan's statistic, typically calculated as
N*R-squared from a regression of the IV residuals on the full set of
instruments.  Under the assumption of conditional homoskedasticity, Hansen's J
statistic becomes Sargan's statistic.  The J statistic is consistent in the
presence of heteroskedasticity and (for HAC-consistent estimation)
autocorrelation; Sargan's statistic is consistent if the disturbance is
homoskedastic and (for AC-consistent estimation) if it is also autocorrelated.
With {cmd:robust}, {cmd:bw()}, and/or {cmd:cluster()}, Hansen's J statistic is
reported.  In the latter case, the statistic allows observations to be
correlated within groups.  For further discussion, see Hayashi (2000, 227-228,
407, and 417).

{p 4 4 2}The Sargan statistic can also be calculated after
{cmd:ivregress} or {cmd:ivreg2} by the command {cmd:overid}.
The features of {cmd:ivreg2} that are unavailable in {cmd:overid}
are the J statistic and the C statistic;
the {cmd:overid} options unavailable in {cmd:ivreg2}
are various small-sample and pseudo-F versions of Sargan's statistic
and its close relative, Basmann's statistic.
See {helpb overid} (if installed).


    {marker s_endog}{title:Testing subsets of regressors and instruments for endogeneity}

{marker ctest}{pstd}The C statistic (also known as a GMM distance or
difference-in-Sargan statistic) implemented using the {cmd:orthog()} option,
allows a test of a subset of the orthogonality conditions; i.e., it is a test
of the exogeneity of one or more instruments.  It is defined as the difference
of the Sargan-Hansen statistic of the equation with the smaller set of
instruments (valid under both the null and alternative hypotheses) and the
equation with the full set of instruments, i.e., including the instruments
whose validity is suspect.  Under the null hypothesis that both the smaller set
of instruments and the additional, suspect instruments are valid, the C
statistic is distributed as chi-squared in the number of instruments tested.
Failure to reject the null hypothesis requires that the full set of
orthogonality conditions be valid; the C statistic and the Sargan-Hansen test
statistics for the equations with both the smaller and full set of instruments
should all be small.  The instruments tested may be either excluded or included
exogenous variables.  If excluded exogenous variables are being tested, the
equation that does not use these orthogonality conditions omits the suspect
instruments from the excluded instruments.  If included exogenous variables are
being tested, the equation that does not use these orthogonality conditions
treats the suspect instruments as included endogenous variables.  To guarantee
that the C statistic is nonnegative in finite samples, the estimated covariance
matrix of the full set orthogonality conditions is used to calculate both
Sargan-Hansen statistics (for simple IV/2SLS, this amounts to using
the MSE from the unrestricted equation to calculate both Sargan statistics).
If estimation is by LIML, the C statistic reported is now based on the
Sargan-Hansen test statistics from the restricted and unrestricted equation.
For further discussion, see Hayashi (2000, 218-222 and 232-234).

{marker endogtest}{pstd}Endogeneity tests of one or more endogenous regressors
can implemented using the {cmd:endogtest()} option.
Under the null hypothesis that the specified endogenous regressors
can actually be treated as exogenous, the test statistic is distributed
as chi-squared with degrees of freedom equal to the number of regressors tested.
The endogeneity test implemented by {cmd:ivreg2} is, like the C statistic,
defined as the difference of two Sargan-Hansen statistics:
one for the equation with the smaller set of instruments,
where the suspect regressor(s) are treated as endogenous,
and one for the equation with the larger set of instruments,
where the suspect regressors are treated as exogenous.
Also like the C statistic, the estimated covariance matrix used
guarantees a nonnegative test statistic.
Under conditional homoskedasticity,
this endogeneity test statistic is numerically equal to
a Hausman test statistic; see Hayashi (2000, 233-234).
The endogeneity test statistic can also be calculated after
{cmd:ivregress} or {cmd:ivreg2} by the command {cmd:ivendog}.
Unlike the Durbin-Wu-Hausman tests reported by {cmd:ivendog},
the {cmd:endogtest()} option of {cmd:ivreg2} can report test statistics
that are robust to various violations of conditional homoskedasticity;
the {cmd:ivendog} option unavailable in {cmd:ivreg2}
is the Wu-Hausman F-test version of the endogeneity test.
See {helpb ivendog} (if installed).


    {marker s_relevance}{title:Tests of under- and weak identification}

{marker idtest}{pstd}{cmd:ivreg2} automatically reports tests of both
underidentification and weak identification.  The underidentification test is
an LM test of whether the equation is identified, i.e., that the excluded
instruments are relevant, meaning correlated with the endogenous regressors.
The test is essentially the test of the rank of a matrix: under the null
hypothesis that the equation is underidentified, the matrix of reduced-form
coefficients on the L1 excluded instruments has rank=K1-1 where K1=number of
endogenous regressors.  Under the null, the statistic is distributed as
chi-squared with degrees of freedom=(L1-K1+1).  A rejection of the null
indicates that the matrix is full column rank; i.e., the model is identified.
When errors are assumed to be i.i.d., {cmd:ivreg2} automatically reports an LM
version of the Anderson (1951) canonical correlations test.  Denoting the
minimum eigenvalue of the canonical correlations as CCEV, the smallest
canonical correlation between the K1 endogenous regressors and the L1 excluded
instruments (after partialling out the K2=L2 exogenous regressors) is
sqrt(CCEV), and the Anderson LM test statistic is N*CCEV, i.e., N times the
square of the smallest canonical correlation.  With the {cmd:first} or
{cmd:ffirst} options, {cmd:ivreg2} also reports the closely related
Cragg-Donald (1993) Wald test statistic.  Again assuming i.i.d. errors and
denoting the minimum eigenvalue of the Cragg-Donald statistic as CDEV,
CDEV=CCEV/(1-CCEV), and the Cragg-Donald Wald statistic is N*CDEV.  Like the
Anderson LM statistic, the Cragg-Donald Wald statistic is distributed as
chi-squared with (L1-K1+1) degrees of freedom.  A result of rejection
of the null should be treated with caution, because weak instrument problems
may still be present.  See Hall, Rudebusch, and Wilcox  (1996) for a discussion
of this test and below for discussion of testing for the presence of weak
instruments.

{pstd}When the i.i.d. assumption is dropped
and {cmd:ivreg2} reports heteroskedastic, AC, HAC,
or cluster-robust statistics,
the Anderson LM and Cragg-Donald Wald statistics are no longer valid.
In these cases, {cmd:ivreg2} reports the LM and Wald versions
of the Kleibergen-Paap (2006) rk statistic,
also distributed as chi-squared with (L1-K1+1) degrees of freedom.
The rk statistic can be seen as a generalization of these tests
to the case of non-i.i.d. errors;
see Kleibergen and Paap (2006) for discussion
and Kleibergen and Schaffer (2007) for a Stata implementation, {cmd:ranktest}.
{cmd:ivreg2} requires {cmd:ranktest} to be installed
and will prompt the user to install it if necessary.
If {cmd:ivreg2} is invoked with the {cmd:robust} option,
the rk underidentification test statistics will be heteroskedasticity-robust
and similarly with {cmd:bw()} and {cmd:cluster()}.

{marker widtest}{pstd}Weak identification arises when the excluded instruments
are correlated with the endogeous regressors but only weakly.  Estimators can
perform poorly when instruments are weak, and different estimators are more
robust to weak instruments (e.g., LIML) than others (e.g., IV); see
Stock and Yogo (2002, 2005) for further discussion.  When errors are assumed to
be i.i.d., the test for weak identification automatically reported by
{cmd:ivreg2} is an F version of the Cragg-Donald Wald statistic, (N-L)/L1*CDEV,
where L is the number of instruments and L1 is the number of excluded
instruments.  Stock and Yogo (2005) have compiled critical values for the
Cragg-Donald F statistic for several different estimators (IV, LIML,
Fuller-LIML), several different definitions of perform poorly (based on bias
and test size), and a range of configurations (up to 100 excluded instruments
and up to 2 or 3 endogenous regressors, depending on the estimator).
{cmd:ivreg2} will report the Stock-Yogo critical values if these are available;
missing values mean that the critical values have not been tabulated or are not
applicable.  See Stock and Yogo (2002, 2005) for details.

{pstd}When the i.i.d. assumption is dropped and {cmd:ivreg2} is invoked with the
{cmd:robust}, {cmd:bw()} or {cmd:cluster()} options, the Cragg-Donald-based
weak instruments test is no longer valid.  {cmd:ivreg2} instead reports a
correspondingly robust Kleibergen-Paap Wald rk F statistic.  The
degrees-of-freedom adjustment for the rk statistic is (N-L)/L1, as with the
Cragg-Donald F statistic, except in the cluster-robust case, when the
adjustment is ((N-L)/L1)*((N-1)/N)*(N_clust-1)/N_clust), following the standard
Stata small-sample adjustment for cluster-robust.  The critical values reported
by {cmd:ivreg2} for the Kleibergen-Paap statistic are the Stock-Yogo critical
values for the Cragg-Donald i.i.d. case.  The critical values reported with
2-step GMM are the Stock-Yogo IV critical values, and the critical values
reported with CUE are the LIML critical values.


    {marker s_redundancy}{title:Testing instrument redundancy}

{marker redtest}{pstd}The {cmd:redundant()} option allows a test of
whether a subset of excluded instruments is redundant.
Excluded instruments are redundant if the asymptotic efficiency
of the estimation is not improved by using them.
Breusch et al. (1999) show that the condition for the redundancy of a set of instruments
can be stated in several equivalent ways:
e.g., in the reduced-form regressions of the endogenous regressors
on the full set of instruments,
the redundant instruments have statistically insignificant coefficients,
or the partial correlations between the endogenous regressors
and the instruments in question are zero.
{cmd:ivreg2} uses a formulation based on testing the rank
of the matrix cross-product between the endogenous regressors
and the possibly redundant instruments after both have
all other instruments partialled-out;
{cmd:ranktest} is used to test whether the matrix has zero rank.
The test statistic is an LM test
and numerically equivalent to a regression-based LM test.
Under the null that the specified instruments are redundant,
the statistic is distributed as chi-squared
with degrees of freedom=(#endogenous regressors)*(#instruments tested).
Rejection of the null indicates that
the instruments are not redundant.
When the i.i.d. assumption is dropped
and {cmd:ivreg2} reports heteroskedastic, AC, HAC,
or cluster-robust statistics,
the redundancy test statistic is similarly robust.
See Baum, Schaffer, and Stillman (2007) for further discussion.

{pstd}Calculation and reporting of all underidentification
and weak identification statistics
can be suppressed with the {cmd:noid} option.


    {marker s_first}{title:First-stage regressions, identification, and weak-id-robust inference}

{marker partialr2}{pstd}The {cmd:first} and {cmd:ffirst} options report
various first-stage results and identification statistics.
Tests of both underidentification and weak identification are reported
(see {help ivreg2##s_relevance:above}).
The first-stage results also include Shea's (1997) partial R-squared measure
of instrument relevance that takes
intercorrelations among instruments into account,
the more common form of partial R-squared
(a.k.a. the squared-partial correlation between the excluded
instruments and the endogenous regressor in question),
and the F test of the excluded instruments
in the corresponding first-stage regression.
When the model has only one endogenous regressor,
the two measures of partial R-squared coincide;
the F-stat form of the Cragg-Donald statistic.
The two partial R-squared measures, the F statistic,
the degrees of freedom of the F statistic,
and the p-value of the F statistic for each endogenous variable
are saved in the matrix {cmd:e(first)}.
The first-stage results are always reported with small-sample statistics,
to be consistent with the recommended use of the first-stage F test as a diagnostic.
If the estimated equation is reported with robust standard errors,
the first-stage F test is also robust.

{marker wirobust}{pstd}The first-stage output also includes
two statistics that provide weak-instrument robust inference
for testing the significance of the endogenous regressors in the structural equation being estimated.
The first statistic is the Anderson-Rubin (1949) test
(not to be confused with the Anderson-Rubin overidentification test for LIML estimation;
see {help ivreg2##s_liml:above}).
The second is the closely related Stock-Wright (2000) S statistic.  The null
hypothesis tested in both cases is that the coefficients of the endogenous
regressors in the structural equation are jointly equal to zero and that the
overidentifying restrictions are also valid.  Both tests are robust to the
presence of weak instruments.  The tests are equivalent to estimating the
reduced form of the equation (with the full set of instruments as regressors)
and testing that the coefficients of the excluded instruments are jointly equal
to zero.  In the form reported by {cmd:ivreg2},the Anderson-Rubin statistic is
a Wald test and the Stock-Wright S statistic is a GMM-distance test.  Both
statistics are distributed as chi-squared with L1 degrees of freedom, where
L1 is the number of excluded instruments.  The traditional F-stat version of
the Anderson-Rubin test is also reported.  See Stock and Watson (2000), Dufour
(2003), Chernozhukov and Hansen (2005), and Kleibergen (2007) for further
discussion.  For related alternative test statistics that are also robust to
weak instruments, see {helpb condivreg} (if installed) and the corresponding
discussion in Moreira and Poi (2003) and Mikusheva and Poi (2006).

{pstd}The {cmd:savefirst} option requests that the individual first-stage
regressions be saved for later access using the {cmd:estimates} command.  If
saved, they can also be displayed using {cmd:first} or {cmd:ffirst} and the
{cmd:ivreg2} replay syntax.  The regressions are saved with the prefix
{cmd:_ivreg2_}, unless the user specifies an alternative prefix with the
{cmd:savefprefix}{cmd:(}{it:prefix}{cmd:)} option.


    {marker s_rf}{title:Reduced-form estimates}

{pstd}The {cmd:rf} option requests that the reduced-form estimation of the
equation be displayed.  The {cmd:saverf} option requests that the reduced-form
estimation is saved for later access using the {cmd:estimates} command.  If
saved, it can also be displayed using the {cmd:rf} and the {cmd:ivreg2} replay
syntax.  The regression is saved with the prefix {cmd:_ivreg2_}, unless the
user specifies an alternative prefix with the
{cmd:saverfprefix}{cmd:(}{it:prefix}{cmd:)} option.


    {marker s_partial}{title:Partialling-out exogenous regressors}

{marker partial}{pstd}The {cmd:partial(}{it:varlist}{cmd:)} option requests that
the exogenous regressors in {it:varlist} are partialled-out from all 
other variables (other regressors and excluded instruments) in the estimation.
If the equation includes a constant, it is also automatically partialled out as
well.  The coefficients corresponding to the regressors in {it:varlist} are not
calculated.  By the Frisch-Waugh-Lovell (FWL) theorem in IV, two-step GMM and
LIML estimation the coefficients for the remaining regressors are the same as
those that would be obtained if the variables were not partialled out.  (NB:
this does not hold for CUE or GMM iterated more than two steps.) The
{cmd:partial()} option is most useful when using {cmd:cluster()} and #clusters <
(#exogenous regressors + #excluded instruments).  In these circumstances, the
covariance matrix of orthogonality conditions S is not of full rank, and
efficient GMM and overidentification tests are infeasible since the optimal
weighting matrix W = {bind:S^-1} cannot be calculated.  The problem can be
addressed by using {cmd:partial()} to partial out enough exogenous regressors for
S to have full rank.  A similar problem arises when the regressors include a
variable that is a singleton dummy, i.e., a variable with one 1 and N-1 zeros
or vice versa, if a robust-covariance matrix is requested.  The singleton dummy
causes the robust covariance matrix estimator to be less than full rank. 
Here partialling-out the variable with the singleton dummy solves the
problem.  Specifying {cmd:partial(_cons)} will cause just the constant to be
partialled-out, i.e., the equation will be estimated in deviations-from-means
form.  When {cmd:ivreg2} is invoked with {cmd:partial()}, it reports test
statistics with the same small-sample adjustments as if estimating without
{cmd:partial()}.  After estimation using the {cmd:partial()} option, the
postestimation {cmd:predict} can be used only to generate residuals, and that
in the current implementation, {cmd:partial()} is not compatible with
endogenous variables or instruments (included or excluded) that use time-series
operators.

{pstd}
{opt fwl(varlist)} partialls-out all regressors in {it:varlist}, except the
endogenous variable.


    {marker s_ols}{title:OLS and heteroskedastic OLS estimation}

{pstd}{cmd:ivreg2} also allows straightforward OLS estimation
by using the same syntax as {cmd:regress}, i.e.,
{cmd:ivreg2 depvar varlist1}.
This can be useful if the user wants to use one of the
features of {cmd:ivreg2} in OLS regression, e.g., AC or
HAC standard errors.

{pstd}If the list of endogenous variables {it:varlist2} is empty
but the list of excluded instruments {it:varlist_iv} is not
and the option {cmd:gmm2s} is specified,
{cmd:ivreg2} calculates Cragg's heteroskedastic OLS (HOLS) estimator,
an estimator that is more efficient than OLS
in the presence of heteroskedasticity of unknown form
(see Davidson and MacKinnon 1993, 599-600).
If the option {cmd:bw(}{it:#}{cmd:)} is specified,
the HOLS estimator is efficient in the presence of
arbitrary autocorrelation;
if both {cmd:bw(}{it:#}{cmd:)} and {cmd:robust} are specified
the HOLS estimator is efficient in the presence of
arbitrary heteroskedasticity and autocorrelation;
and if {cmd:cluster(}{it:varname}{cmd:)} is used,
the HOLS estimator is efficient in the presence of
arbitrary heteroskedasticity and within-group correlation.
The efficiency gains of HOLS derive from the orthogonality conditions
of the excluded instruments listed in {it:varlist_iv}.
If no endogenous variables are specified and {cmd:gmm2s} is not specified,
{cmd:ivreg2} reports standard OLS coefficients.
The Sargan-Hansen statistic reported
when the list of endogenous variables {it:varlist2} is empty
is a Lagrange multiplier (LM) test
of the hypothesis that the excluded instruments {it:varlist_iv} are
correctly excluded from the restricted model.
If the estimation is LIML, the LM statistic reported
is now based on the Sargan-Hansen test statistics from
the restricted and unrestricted equation.
For more on LM tests, see Wooldridge (2002, 58-60).
Because the approach of the HOLS estimator
has applications beyond heteroskedastic disturbances,
and to avoid confusion concerning the robustness of the estimates,
the estimators presented above as HOLS
are described in the output of {cmd:ivreg2}
as 2-Step GMM, CUE, etc., as appropriate.


    {marker s_collin}{title:Collinearities}

{pstd}{cmd:ivreg2} checks the lists of included instruments,
excluded instruments, and endogenous regressors
for collinearities and duplicates. If an endogenous regressor is
collinear with the instruments, it is reclassified as exogenous. If any
endogenous regressors are collinear with each other, some are dropped.
If there are any collinearities among the instruments, some are dropped.
In Stata 9 and later, excluded instruments are dropped before included
instruments.  If any variables are dropped, a list of their names are saved in
the macros {cmd:e(collin)} and/or {cmd:e(dups)}.  Lists of the included and
excluded instruments and the endogenous regressors with collinear variables and
duplicates removed are also saved in macros with "{cmd:1}" appended to the
corresponding macro names.

{pstd}Collinearity checks can be suppressed with the {cmd:nocollin} option.


    {marker s_speed}{title:Speed options: nocollin and noid}

{pstd}Two options are available for speeding execution.
{cmd:nocollin} specifies that the collinearity checks not be performed.
{cmd:noid} suspends calculation and reporting of
the underidentification and weak identification statistics
in the main output.


    {marker s_small}{title:Small-sample corrections}

{pstd}Mean square error = sqrt(RSS/(N-K)) if {cmd:small}, = sqrt(RSS/N) otherwise.

{pstd}If {cmd:robust} is chosen, the finite sample adjustment
(see {hi:[R] regress}) to the robust variance-covariance matrix
qc = N/(N-K) if {cmd:small}, qc = 1 otherwise.

{pstd}If {cmd:cluster()} is chosen, the finite sample adjustment
qc = (N-1)/(N-K)*M/(M-1) if {cmd:small}, where M=number of clusters,
qc = 1 otherwise.

{pstd}The Sargan and C (difference-in-Sargan) statistics use
error variance = RSS/N; i.e., there is no small sample correction.

{pstd}A full discussion of these computations and related topics
can be found in Baum, Schaffer, and Stillman (2003) and Baum, Schaffer and
Stillman (2007). Some features of the program postdate the former article and are described in the latter paper.


{marker s_options}{title:Options}

{p 4 8 2}{cmd:gmm2s} requests the two-step efficient GMM estimator.  If no
endogenous variables are specified, the estimator is Cragg's HOLS estimator.

{p 4 8 2}{cmd:liml} requests the LIML estimator.

{p 4 8 2}{cmd:fuller(}{it:#}{cmd:)} specifies that Fuller's modified LIML
estimator is calculated using the user-supplied Fuller parameter alpha, a
nonnegative number.  Alpha equal to 1 has been suggested as a good choice.

{p 4 8 2}{cmd:kclass(}{it:#}{cmd:)} specifies that a general k-class estimator
is calculated using the user-supplied {it:#}, a nonnegative number.

{p 4 8 2}{cmd:coviv} specifies that the matrix used to calculate the covariance
matrix for the LIML or k-class estimator is based on the 2SLS matrix, i.e.,
with k=1.  Here the covariance matrix will differ from that calculated for the
2SLS estimator only because the estimate of the error variance will differ.
The default is for the covariance matrix to be based on the LIML or k-class
matrix.

{p 4 8 2}{cmd:cue} requests the GMM continuously updated estimator (CUE).

{p 4 8 2}{cmd:cueinit(}{it:matrix}{cmd:)} specifies that the starting values
for the CUE estimator use those in a user-supplied matrix b.  If omitted, the
default behavior is to use starting values from IV or 2-step efficient GMM
estimation.

{p 4 8 2}{cmd:cueopt(}{it:string}{cmd:)} passes user-specified options
to Stata's {cmd:ml} routine; see {manhelp ml R}.

{p 4 8 2}{cmd:b0(}{it:matrix}{cmd:)} specifies that the J statistic
(i.e., the value of the CUE objective function)
should be calculated for an arbitrary coefficient vector {cmd:b0()}.
That vector must be provided as a matrix with appropriate row and column names.
Under- and weak-identification statistics are not reported
in the output.

{p 4 8 2}{cmd:robust} specifies that the Eicker/Huber/White/sandwich estimator
of variance is to be used in place of the traditional calculation.
{cmd:robust} combined with {cmd:cluster()} further allows residuals which are
not independent within cluster (although they must be independent between
clusters).  See {hi:[U] Obtaining robust variance estimates}.

{p 4 8 2}{cmd:cluster}{cmd:(}{it:varname}{cmd:)} specifies that the
observations are independent across groups (clusters) but not necessarily
independent within groups.  {it:varname} specifies to which group each
observation belongs; e.g., {cmd:cluster(personid)} in data with repeated
observations on individuals.  {cmd:cluster()} can be used with {help pweight}s
to produce estimates for unstratified cluster-sampled data, but 
{cmd:svy: regress} is a command especially designed for survey data;
see {manhelp regress R}.  Specifying {cmd:cluster()} implies {cmd:robust}.

{p 4 8 2}{cmd:bw(}{it:#}{cmd:)} implements AC or HAC covariance estimation
with bandwidth equal to {it:#}, where {it:#} is an integer greater than zero.
Specifying {cmd:robust} implements HAC covariance estimation;
omitting it implements AC covariance estimation.
If the Bartlett (default), Parzen, or quadratic-spectral kernels are selected,
the value {cmd:auto} may be given (rather than an integer)
to invoke Newey and West's (1994) automatic bandwidth-selection procedure.

{p 4 8 2}{cmd:kernel(}{it:string}{cmd:)} specifies the kernel to be used for AC
and HAC covariance estimation; the default kernel is {cmdab:bar:tlett} (also
known in econometrics as Newey-West).  Other kernels available are
{cmdab:tru:ncated}; {cmdab:par:zen};
{cmdab:tukey-han:ning} ({cmd:thann} is a synonym);
{cmdab:tukey-ham:ming} ({cmd:thamm}) is a synonym);
{cmd:dan}iell; {cmd:tent}; and
{cmd:qua:dratic-spectral} ({cmd:qs} is a synonym).

{p 4 4}Note: for {cmd:bartlett}, {cmd:parzen},
and {cmd:tukey-hanning} and tukey-hamming} kernels, the number of lags used
to construct the kernel estimate equals the bandwidth minus one.
Stata's official {cmd:newey} implements
HAC standard errors based on the Bartlett kernel
and requires the user to specify
the maximum number of lags used and not the bandwidth;
see {manhelp newey R}.
If these kernels are used with {cmd:bw(1)},
no lags are used and {cmd:ivreg2} will report the usual
Eicker/Huber/White/sandwich variance estimates.

{p 4 8 2}{cmd:wmatrix(}{it:matrix}{cmd:)} specifies a user-supplied weighting matrix
in place of the computed optimal weighting matrix.
The matrix must be positive definite.
The user-supplied matrix must have the same row and column names
as the IV in the regression model (or a subset thereof). 

{p 4 8 2}{cmd:smatrix(}{it:matrix}{cmd:)} specifies a user-supplied covariance matrix
of the orthogonality conditions to be used in calculating the covariance matrix of the estimator.
The matrix must be positive definite.
The user-supplied matrix must have the same row and column names
as the instrument variables in the regression model (or a subset thereof).  

{p 4 8 2}{cmd:orthog}{cmd:(}{it:varlist_ex}{cmd:)} requests that a C statistic
be calculated as a test of the exogeneity of the instruments in {it:varlist_ex}.
These may be either included or excluded exogenous variables.
The standard order condition for identification applies:
the restricted equation that does not use these variables
as exogenous instruments must still be identified.

{p 4 8 2}{cmd:endogtest}{cmd:(}{it:varlist_en}{cmd:)} requests that a C
statistic be calculated as a test of the endogeneity of the endogenous
regressors in {it:varlist_en}.

{p 4 8 2}{cmd:redundant}{cmd:(}{it:varlist_ex}{cmd:)} requests an LM test
of the redundancy of the instruments in {it:varlist_ex}.
These must be excluded exogenous variables.
The standard order condition for identification applies:
the restricted equation that does not use these variables
as exogenous instruments must still be identified.

{p 4 8 2}{cmd:small} requests that small-sample statistics (F and t statistics)
be reported instead of large-sample statistics (chi-squared and z statistics).
Large-sample statistics are the default.
The exception is the statistic for the significance of the regression,
which is always reported as a small-sample F statistic.

{p 4 8 2}{cmd:noconstant} suppresses the constant term (intercept) in the
regression.  If {cmd:noconstant} is specified, the constant term is excluded
from both the final regression and the first-stage regression.  To include a
constant in the first-stage when {cmd:noconstant} is specified, explicitly
include a variable containing all 1s in {it:varlist_iv}.

{p 4 8 2}{cmd:first} requests that the full first-stage regression results be
displayed, along with the associated diagnostic and identification statistics.

{p 4 8 2}{cmd:ffirst} requests the first-stage diagnostic and identification
statistics.  The results are saved in various {cmd:e()} macros.

{p 4 8 2}{cmd:nocollin} suppresses the checks for collinearities
and duplicate variables.

{p 4 8 2}{cmd:noid} suppresses the calculation and reporting
of underidentification and weak identification statistics.

{p 4 8 2}{cmd:savefirst} requests that the first-stage regressions results
are saved for later access using the {cmd:estimates} command.
The names under which the first-stage regressions are saved
are the names of the endogenous regressors prefixed by {cmd:_ivreg2_}.
If these use Stata's time-series operators,
the "{cmd:.}" is replaced by a "{cmd:_}".
The maximum number of first-stage estimation results that can be saved
depends on how many other estimation results the user has already saved
and on the maximum supported by Stata (20 for Stata 8.2 and 9.0, 300 for Stata 9.1 and later).

{p 4 8 2}{cmd:savefprefix}{cmd:(}{it:prefix}{cmd:)} requests that
the first-stage regression results be saved using the user-specified prefix
instead of the default "{cmd:_ivreg2_}".

{p 4 8 2}{cmd:rf} requests that the reduced-form estimation of the equation
be displayed.

{p 4 8 2}{cmd:saverf} requests that the reduced-form estimation of the equation
be saved for later access using the {cmd:estimates} command.
The estimation is stored under the name of the dependent variable
prefixed by {cmd:_ivreg2_}.
If this uses Stata's time-series operators,
the "{cmd:.}" is replaced by a "{cmd:_}".

{p 4 8 2}{cmd:saverfprefix}{cmd:(}{it:prefix}{cmd:)} requests that
the reduced-form estimation be saved using the user-specified prefix
instead of the default {cmd:_ivreg2_}.

{p 4 8 2}{cmd:partial(}{it:varlist}{cmd:)} requests that
the exogenous regressors in {it:varlist} be partialled out
from the other variables in the equation.
If the equation includes a constant,
it is automatically partialled out as well. 
The coefficients corresponding to the regressors in {it:varlist}
are not calculated.

{p 4 8 2}{cmd:level(}{it:#}{cmd:)} specifies the confidence level, as a
percentage, for confidence intervals of the coefficients; see
{helpb level}.

{p 4 8 2}{cmd:noheader}, {cmd:eform()}, {cmd:depname()} and {cmd:plus}
are for ado-file writers; see {hi:[R] ivregress} and {hi:[R] regress}.

{p 4 8 2}{cmd:nofooter} suppresses the display of the footer containing
identification and overidentification statistics, exogeneity and endogeneity
tests, lists of endogenous variables and instruments, etc.

{p 4 8 2}{cmd:version} causes {cmd:ivreg2} to display its current version
number and to leave it in the macro {cmd:e(version)}.  It cannot be used with
any other options and will clear any existing {cmd:e()} saved results.


{marker s_remarks}{title:Remarks}

{pstd}{cmd:ivreg2} does not report an ANOVA table.
Instead, it reports the RSS and both the centered and uncentered TSS.
It also reports both the centered and uncentered R-squared.
NB: the TSS and R-squared reported by official {cmd:ivregress} is centered
if a constant is included in the regression, and uncentered otherwise.


{marker s_macros}{title:Saved results}

{pstd}{cmd:ivreg2} saves the following results in {cmd:e()}:

{synoptset 20 tabbed}{...}
{p2col 5 20 24 2: Scalars}{p_end}
{synopt:{cmd:e(N)}}number of observations{p_end}
{synopt:{cmd:e(yy)}}total sum of squares (SS), uncentered (y'y){p_end}
{synopt:{cmd:e(yyc)}}total SS, centered (y'y - ((1'y)^2)/n){p_end}
{synopt:{cmd:e(rss)}}residual SS{p_end}
{synopt:{cmd:e(mss)}}model SS =yyc-rss if the eqn has a constant, =yy-rss otherwise{p_end}
{synopt:{cmd:e(df_m)}}model degrees of freedom{p_end}
{synopt:{cmd:e(df_r)}}residual degrees of freedom{p_end}
{synopt:{cmd:e(r2u)}}uncentered R-squared, 1-rss/yy{p_end}
{synopt:{cmd:e(r2c)}}centered R-squared, 1-rss/yyc{p_end}
{synopt:{cmd:e(r2)}}centered R-squared if the eqn has a constant, uncentered otherwise{p_end}
{synopt:{cmd:e(r2_a)}}adjusted R-squared{p_end}
{synopt:{cmd:e(ll)}}log likelihood{p_end}
{synopt:{cmd:e(rankxx)}}rank of the matrix of observations on rhs variables=K{p_end}
{synopt:{cmd:e(rankzz)}}rank of the matrix of observations on instruments=L{p_end}
{synopt:{cmd:e(rankV)}}rank of covariance matrix V of coefficients{p_end}
{synopt:{cmd:e(rankS)}}rank of covariance matrix S of orthogonality conditions{p_end}
{synopt:{cmd:e(rmse)}}root mean square error=sqrt(rss/(N-K)) if {cmd:small}, =sqrt(rss/N) otherwise{p_end}
{synopt:{cmd:e(F)}}F statistic{p_end}
{synopt:{cmd:e(N_clust)}}number of clusters{p_end}
{synopt:{cmd:e(bw)}}bandwidth{p_end}
{synopt:{cmd:e(lambda)}}LIML eigenvalue{p_end}
{synopt:{cmd:e(kclass)}}k in k-class estimation{p_end}
{synopt:{cmd:e(fuller)}}Fuller parameter alpha{p_end}
{synopt:{cmd:e(sargan)}}Sargan statistic{p_end}
{synopt:{cmd:e(sarganp)}}p-value of Sargan statistic{p_end}
{synopt:{cmd:e(sargandf)}}dof of Sargan statistic = degree of overidentification = L-K{p_end}
{synopt:{cmd:e(j)}}Hansen J statistic{p_end}
{synopt:{cmd:e(jp)}}p-value of Hansen J statistic{p_end}
{synopt:{cmd:e(jdf)}}dof of Hansen J statistic = degree of overidentification = L-K{p_end}
{synopt:{cmd:e(arubin)}}Anderson-Rubin overidentification LR statistic{p_end}
{synopt:{cmd:e(arubinp)}}p-value of Anderson-Rubin overidentification LR statistic{p_end}
{synopt:{cmd:e(arubindf)}}dof of A-R overid statistic = degree of overidentification = L-K{p_end}
{synopt:{cmd:e(idstat)}}LM test statistic for underidentification (Anderson or Kleibergen-Paap){p_end}
{synopt:{cmd:e(idp)}}p-value of underidentification LM statistic{p_end}
{synopt:{cmd:e(iddf)}}dof of underidentification LM statistic{p_end}
{synopt:{cmd:e(widstat)}}F statistic for weak identification (Cragg-Donald or Kleibergen-Paap){p_end}
{synopt:{cmd:e(arf)}}Anderson-Rubin F test of significance of endogenous regressors{p_end}
{synopt:{cmd:e(arfp)}}p-value of Anderson-Rubin F test of endogenous regressors{p_end}
{synopt:{cmd:e(archi2)}}Anderson-Rubin chi-sqared test of significance of endogenous regressors{p_end}
{synopt:{cmd:e(archi2p)}}p-value of Anderson-Rubin chi-sqared test of endogenous regressors{p_end}
{synopt:{cmd:e(ardf)}}degrees of freedom of Anderson-Rubin tests of endogenous regressors{p_end}
{synopt:{cmd:e(ardf_r)}}denominator degrees of freedom of AR F test of endogenous regressors{p_end}
{synopt:{cmd:e(redstat)}}LM statistic for instrument redundancy{p_end}
{synopt:{cmd:e(redp)}}p-value of LM statistic for instrument redundancy{p_end}
{synopt:{cmd:e(reddf)}}dof of LM statistic for instrument redundancy{p_end}
{synopt:{cmd:e(cstat)}}C statistic{p_end}
{synopt:{cmd:e(cstatp)}}p-value of C statistic{p_end}
{synopt:{cmd:e(cstatdf)}}degrees of freedom of C statistic{p_end}
{synopt:{cmd:e(cons)}}{cmd:1} when equation has a Stata-supplied constant; {cmd:0} otherwise{p_end}
{synopt:{cmd:e(partialcons)}}as above but prior to partialling-out (see {cmd:e(partial)}){p_end}
{synopt:{cmd:e(partial_ct)}}number of partialled-out variables (see {cmd:e(partial)}){p_end}

{p2col 5 20 24 2: Macros}{p_end}
{synopt:{cmd:e(cmd)}}{cmd:ivreg2}{p_end}
{synopt:{cmd:e(cmdline)}}command line invoking {cmd:ivreg2}{p_end}
{synopt:{cmd:e(version)}}version number of {cmd:ivreg2}{p_end}
{synopt:{cmd:e(model)}}{cmd:ols}, {cmd:iv}, {cmd:gmm}, {cmd:liml}, or {cmd:kclass}{p_end}
{synopt:{cmd:e(depvar)}}name of dependent variable{p_end}
{synopt:{cmd:e(instd)}}instrumented (RHS endogenous) variables{p_end}
{synopt:{cmd:e(insts)}}instruments{p_end}
{synopt:{cmd:e(inexog)}}included instruments (regressors){p_end}
{synopt:{cmd:e(exexog)}}excluded instruments{p_end}
{synopt:{cmd:e(collin)}}variables dropped because of collinearities{p_end}
{synopt:{cmd:e(dups)}}duplicate variables{p_end}
{synopt:{cmd:e(ecollin)}}endogenous variables reclassified as exogenous because of collinearities with instruments{p_end}
{synopt:{cmd:e(clist)}}instruments tested for orthogonality{p_end}
{synopt:{cmd:e(redlist)}}instruments tested for redundancy{p_end}
{synopt:{cmd:e(partial)}}partialled-out exogenous regressors{p_end}
{synopt:{cmd:e(small)}}{cmd:small}{p_end}
{synopt:{cmd:e(wtype)}}weight type{p_end}
{synopt:{cmd:e(wexp)}}weight expression{p_end}
{synopt:{cmd:e(clustvar)}}name of cluster variable{p_end}
{synopt:{cmd:e(vcetype)}}covariance estimation method{p_end}
{synopt:{cmd:e(kernel)}}kernel{p_end}
{synopt:{cmd:e(tvar)}}time variable{p_end}
{synopt:{cmd:e(ivar)}}panel variable{p_end}
{synopt:{cmd:e(firsteqs)}}names of stored first-stage equations{p_end}
{synopt:{cmd:e(rfeq)}}name of stored reduced-form equation{p_end}
{synopt:{cmd:e(predict)}}program used to implement {cmd:predict}{p_end}

{p2col 5 20 24 2: Matrices}{p_end}
{synopt:{cmd:e(b)}}coefficient vector{p_end}
{synopt:{cmd:e(V)}}variance-covariance matrix of the estimators{p_end}
{synopt:{cmd:e(S)}}covariance matrix of orthogonality conditions{p_end}
{synopt:{cmd:e(W)}}GMM weighting matrix (=inverse of S if efficient GMM estimator){p_end}
{synopt:{cmd:e(first)}}first-stage regression results{p_end}
{synopt:{cmd:e(ccev)}}eigenvalues corresponding to the Anderson canonical correlations test{p_end}
{synopt:{cmd:e(cdev)}}eigenvalues corresponding to the Cragg-Donald test{p_end}

{p2col 5 20 24 2: Functions}{p_end}
{synopt:{cmd:e(sample)}}marks estimation sample{p_end}
{p2colreset}{...}


{marker s_examples}{title:Examples}

{p 8 12 2}{stata "use http://fmwww.bc.edu/ec-p/data/hayashi/griliches76.dta" : . use http://fmwww.bc.edu/ec-p/data/hayashi/griliches76.dta }{p_end}

{p 8 12 2}{stata "xi i.year" : . xi i.year}

{pstd}(Instrumental variables.  Examples follow Hayashi 2000, 255.)

{p 8 12 2}{stata "ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age mrt)" : . ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age mrt)}

{p 8 12 2}{stata "ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age mrt), small ffirst" : . ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age mrt), small ffirst}

{pstd}(Testing for the presence of heteroskedasticity in IV/GMM estimation)

{p 8 12 2}{stata "ivhettest, fitlev" : . ivhettest, fitlev}

{pstd}(Two-step GMM efficient in the presence of arbitrary heteroskedasticity)

{p 8 12 2}{stata "ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age mrt), gmm2s robust" : . ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age mrt), gmm2s robust}

{pstd}(GMM with user-specified first-step weighting matrix or matrix of orthogonality conditions)

{p 8 12 2}{stata "ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age mrt), robust" : . ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age mrt), robust}

{p 8 12 2}{stata "predict double uhat if e(sample), resid" : . predict double uhat if e(sample), resid}

{p 8 12 2}{stata "mat accum S =  `e(insts)' [iw=uhat^2]" : . mat accum S = `e(insts)' [iw=uhat^2]}

{p 8 12 2}{stata "mat S = 1/`e(N)' * S" : . mat S = 1/`e(N)' * S}

{p 8 12 2}{stata "ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age mrt), gmm2s robust smatrix(S)" : . ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age mrt), gmm2s robust smatrix(S)}

{p 8 12 2}{stata "mat W = invsym(S)" : . mat W = invsym(S)}

{p 8 12 2}{stata "ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age mrt), gmm2s robust wmatrix(W)" : . ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age mrt), gmm2s robust wmatrix(W)}

{pstd}(Equivalence of J statistic and Wald tests of included regressors, irrespective of instrument choice [Ahn 1997])

{p 8 12 2}{stata "ivreg2 lw (iq=med kww age), gmm2s" : . ivreg2 lw (iq=med kww age), gmm2s}

{p 8 12 2}{stata "mat S0 = e(S)" : . mat S0 = e(S)}

{p 8 12 2}{stata "qui ivreg2 lw (iq=kww) med age, gmm2s smatrix(S0)" : . qui ivreg2 lw (iq=kww) med age, gmm2s smatrix(S0)}

{p 8 12 2}{stata "test med age" : . test med age}

{p 8 12 2}{stata "qui ivreg2 lw (iq=med) kww age, gmm2s smatrix(S0)" : . qui ivreg2 lw (iq=med) kww age, gmm2s smatrix(S0)}

{p 8 12 2}{stata "test kww age" : . test kww age}

{p 8 12 2}{stata "qui ivreg2 lw (iq=age) med kww, gmm2s smatrix(S0)" : . qui ivreg2 lw (iq=age) med kww, gmm2s smatrix(S0)}

{p 8 12 2}{stata "test med kww" : . test med kww}

{pstd}(Continuously-updated GMM (CUE) efficient in the presence of arbitrary heteroskedasticity.  NB: may require 50+ iterations.)

{p 8 12 2}{stata "ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age mrt), cue robust" : . ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age mrt), cue robust}

{pstd}(Continuously-updated GMM (CUE) with ml options)

{p 8 12 2}{stata "ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age mrt), cue robust cueopt(technique(dfp))" : . ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age mrt), cue robust cueopt(technique(dfp))}

{pstd}(Sargan-Basmann tests of overidentifying restrictions for IV estimation)

{p 8 12 2}{stata "ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age mrt)" : . ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age mrt)}

{p 8 12 2}{stata "overid, all" : . overid, all}

{pstd}(Tests of exogeneity and endogeneity)

{pstd}(Test the exogeneity of one regressor)

{p 8 12 2}{stata "ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age mrt), gmm2s orthog(s)" : . ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age mrt), gmm2s orthog(s)}

{pstd}(Test the exogeneity of two excluded instruments)

{p 8 12 2}{stata "ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age mrt), gmm2s orthog(age mrt)" : . ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age mrt), gmm2s orthog(age mrt)}

{pstd}(Frisch-Waugh-Lovell (FWL): equivalence of estimations with and without partialling-out)

{p 8 12 2}{stata "ivreg2 lw s expr tenure rns _I* (iq=kww age), cluster(year)" : . ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age), cluster(year)}

{p 8 12 2}{stata "ivreg2 lw s expr tenure rns _I* (iq=kww age), cluster(year) partial(_I*)" : . ivreg2 lw s expr tenure rns smsa _I* (iq=med kww age), cluster(year) partial(_I*)}

{pstd}({cmd:partial()}: efficient GMM with #clusters<#instruments feasible after partialling-out)

{p 8 12 2}{stata "ivreg2 lw s expr tenure rns _I* (iq=kww age), cluster(year) partial(_I*) gmm2s" : . ivreg2 lw s expr tenure rns smsa (iq=med kww age), cluster(year) partial(_I*) gmm2s}

{pstd}(Examples following Wooldridge 2002, 59 and 61)

{p 8 12 2}{stata "use http://fmwww.bc.edu/ec-p/data/wooldridge/mroz.dta" : . use http://fmwww.bc.edu/ec-p/data/wooldridge/mroz.dta }

{pstd}(Equivalence of DWH endogeneity test when regressor is endogenous...)

{p 8 12 2}{stata "ivreg2 lwage exper expersq (educ=age kidslt6 kidsge6)" : . ivreg2 lwage exper expersq (educ=age kidslt6 kidsge6)}

{p 8 12 2}{stata "ivendog educ" :. ivendog educ}

{pstd}(... endogeneity test using the {cmd:endogtest()} option)

{p 8 12 2}{stata "ivreg2 lwage exper expersq (educ=age kidslt6 kidsge6), endog(educ)" : . ivreg2 lwage exper expersq (educ=age kidslt6 kidsge6), endog(educ)}

{pstd}(...and C test of exogeneity when regressor is exogenous, using the {cmd:orthog()} option)

{p 8 12 2}{stata "ivreg2 lwage exper expersq educ (=age kidslt6 kidsge6), orthog(educ)" : . ivreg2 lwage exper expersq educ (=age kidslt6 kidsge6), orthog(educ)}

{pstd}(HOLS)

{p 8 12 2}{stata "ivreg2 lwage exper expersq educ (=age kidslt6 kidsge6), gmm2s" : . ivreg2 lwage exper expersq educ (=age kidslt6 kidsge6), gmm2s}

{pstd}(Equivalence of Cragg-Donald Wald F statistic and F test from first-stage
regression in special case of single endogenous regressor.  Also illustrates
{cmd:savefirst} option.)

{p 8 12 2}{stata "ivreg2 lwage exper expersq (educ=age kidslt6 kidsge6), savefirst" : . ivreg2 lwage exper expersq (educ=age kidslt6 kidsge6), savefirst}

{p 8 12 2}{stata "di e(widstat)" : . di e(widstat)}

{p 8 12 2}{stata "estimates restore _ivreg2_educ" : . estimates restore _ivreg2_educ}

{p 8 12 2}{stata "test age kidslt6 kidsge6" : . test age kidslt6 kidsge6}

{p 8 12 2}{stata "di r(F)" : . di r(F)}

{pstd}(Equivalence of Kleibergen-Paap robust rk Wald F statistic and F test
from first-stage regression in special case of single endogenous regressor.)

{p 8 12 2}{stata "ivreg2 lwage exper expersq (educ=age kidslt6 kidsge6), robust savefirst" : . ivreg2 lwage exper expersq (educ=age kidslt6 kidsge6), robust savefirst}

{p 8 12 2}{stata "di e(widstat)" : . di e(widstat)}

{p 8 12 2}{stata "estimates restore _ivreg2_educ" : . estimates restore _ivreg2_educ}

{p 8 12 2}{stata "test age kidslt6 kidsge6" : . test age kidslt6 kidsge6}

{p 8 12 2}{stata "di r(F)" : . di r(F)}

{pstd}(Equivalence of Kleibergen-Paap robust rk LM statistic for identification
and LM test of joint significance of excluded instruments in first-stage
regression in special case of single endogenous regressor.  Also illustrates
use of {cmd:ivreg2} to perform an LM test in OLS estimation.)

{p 8 12 2}{stata "ivreg2 lwage exper expersq (educ=age kidslt6 kidsge6), robust" : . ivreg2 lwage exper expersq (educ=age kidslt6 kidsge6), robust}

{p 8 12 2}{stata "di e(idstat)" : . di e(idstat)}

{p 8 12 2}{stata "ivreg2 educ exper expersq (=age kidslt6 kidsge6) if e(sample), robust" : . ivreg2 educ exper expersq (=age kidslt6 kidsge6) if e(sample), robust}

{p 8 12 2}{stata "di e(j)" : . di e(j)}

{pstd}(Equivalence of an LM test of an excluded instrument for redundancy and
an LM test of significance from first-stage regression in special case of
single endogenous regressor.)

{p 8 12 2}{stata "ivreg2 lwage exper expersq (educ=age kidslt6 kidsge6), robust redundant(age)" : . ivreg2 lwage exper expersq (educ=age kidslt6 kidsge6), robust redundant(age)}

{p 8 12 2}{stata "di e(redstat)" : . di e(redstat)}

{p 8 12 2}{stata "ivreg2 educ exper expersq kidslt6 kidsge6 (=age) if e(sample), robust" : . ivreg2 educ exper expersq kidslt6 kidsge6 (=age) if e(sample), robust}

{p 8 12 2}{stata "di e(j)" : . di e(j)}

{pstd}(Weak-instrument robust inference: Anderson-Rubin Wald F and chi-squared
and Stock-Wright S statistics.  Also illustrates use of {cmd:saverf} option.)

{p 8 12 2}{stata "ivreg2 lwage exper expersq (educ=age kidslt6 kidsge6), robust ffirst saverf" : . ivreg2 lwage exper expersq (educ=age kidslt6 kidsge6), robust ffirst saverf}

{p 8 12 2}{stata "di e(arf)" : . di e(arf)}

{p 8 12 2}{stata "di e(archi2)" : . di e(archi2)}

{p 8 12 2}{stata "di e(sstat)" : . di e(sstat)}

{pstd}(Obtaining the Anderson-Rubin Wald F statistic from the reduced-form
estimation)

{p 8 12 2}{stata "estimates restore _ivreg2_lwage" : . estimates restore _ivreg2_lwage}

{p 8 12 2}{stata "test age kidslt6 kidsge6" : . test age kidslt6 kidsge6}

{p 8 12 2}{stata "di r(F)" : . di r(F)}

{pstd}(Obtaining the Anderson-Rubin Wald chi-sq statistic from the reduced-form
estimation.  Use {cmd:ivreg2} without {cmd:small} to obtain large-sample test
statistic.)

{p 8 12 2}{stata "ivreg2 lwage exper expersq age kidslt6 kidsge6, robust" : . ivreg2 lwage exper expersq age kidslt6 kidsge6, robust}

{p 8 12 2}{stata "test age kidslt6 kidsge6" : . test age kidslt6 kidsge6}

{p 8 12 2}{stata "di r(chi2)" : . di r(chi2)}

{pstd}(Obtaining the Stock-Wright S statistic as an LM test from a reduced-form-like estimation.)

{p 8 12 2}{stata "ivreg2 lwage exper expersq (=age kidslt6 kidsge6), robust" : . ivreg2 lwage exper expersq (=age kidslt6 kidsge6), robust}

{p 8 12 2}{stata "di e(j)" : . di e(j)}

{pstd}(Obtaining the Stock-Wright S statistic as the value of the CUE/GMM
objective function.  Also illustrates use of {cmd:b0()} option.)

{p 8 12 2}{stata "mat b = 0" : . mat b = 0}

{p 8 12 2}{stata "mat colnames b = educ" : . mat colnames b = educ}

{p 8 12 2}{stata "ivreg2 lwage exper expersq (educ=age kidslt6 kidsge6), robust b0(b) partial(exper expersq)" : . ivreg2 lwage exper expersq (educ=age kidslt6 kidsge6), robust b0(b) partial(exper expersq)}

{p 8 12 2}{stata "di e(j)" : . di e(j)}

{pstd}(LIML and k-class estimation using Klein data)

{col 9}{stata "use http://fmwww.bc.edu/repec/bocode/k/kleinI" :. use http://fmwww.bc.edu/repec/bocode/k/kleinI}

{pstd}(LIML estimates of Klein's consumption function)

{p 8 12 2}{stata "ivreg2 consump L.profit (profit wages = govt taxes trend wagegovt capital1 L.demand), liml" :. ivreg2 consump L.profit (profit wages = govt taxes trend wagegovt capital1 L.demand), liml}

{pstd}(Equivalence of LIML and CUE+homoskedasticity+independence)

{p 8 12 2}{stata "ivreg2 consump L.profit (profit wages = govt taxes trend wagegovt capital1 L.demand), liml coviv" :. ivreg2 consump L.profit (profit wages = govt taxes trend wagegovt capital1 L.demand), liml coviv}

{p 8 12 2}{stata "ivreg2 consump L.profit (profit wages = govt taxes trend wagegovt capital1 L.demand), cue" :. ivreg2 consump L.profit (profit wages = govt taxes trend wagegovt capital1 L.demand), cue}

{pstd}(Fuller's modified LIML with alpha=1)

{p 8 12 2}{stata "ivreg2 consump L.profit (profit wages = govt taxes trend wagegovt capital1 L.demand), fuller(1)" :. ivreg2 consump L.profit (profit wages = govt taxes trend wagegovt capital1 L.demand), fuller(1)}

{pstd}(k-class estimation with Nagar's bias-adjusted IV, k=1+(L-K)/N=1+4/21=1.19)

{p 8 12 2}{stata "ivreg2 consump L.profit (profit wages = govt taxes trend wagegovt capital1 L.demand), kclass(1.19)" :. ivreg2 consump L.profit (profit wages = govt taxes trend wagegovt capital1 L.demand), kclass(1.19)}

{pstd}(Kernel-based covariance estimation using time-series data)

{p 8 12 2}{stata "use http://fmwww.bc.edu/ec-p/data/wooldridge/phillips.dta" :. use http://fmwww.bc.edu/ec-p/data/wooldridge/phillips.dta}

{p 8 12 2}{stata "tsset year, yearly" :. tsset year, yearly}

{pstd}(Autocorrelation-consistent (AC) inference in an OLS Regression)

{p 8 12 2}{stata "ivreg2 cinf unem, bw(3)" :. ivreg2 cinf unem, bw(3)}

{p 8 12 2}{stata "ivreg2 cinf unem, kernel(qs) bw(auto)" :. ivreg2 cinf unem, kernel(qs) bw(auto)}

{pstd}(Heteroskedastic and autocorrelation-consistent (HAC) inference in an OLS regression)

{p 8 12 2}{stata "ivreg2 cinf unem, bw(3) kernel(bartlett) robust small" :. ivreg2 cinf unem, bw(3) kernel(bartlett) robust small}

{p 8 12 2}{stata "newey cinf unem, lag(2)" :. newey cinf unem, lag(2)}

{pstd}(AC and HAC in IV and GMM estimation)

{p 8 12 2}{stata "ivreg2 cinf (unem = l(1/3).unem), bw(3)" :. ivreg2 cinf (unem = l(1/3).unem), bw(3)}

{p 8 12 2}{stata "ivreg2 cinf (unem = l(1/3).unem), bw(3) gmm2s kernel(thann)" :. ivreg2 cinf (unem = l(1/3).unem), bw(3) gmm2s kernel(thann)}

{p 8 12 2}{stata "ivreg2 cinf (unem = l(1/3).unem), bw(3) gmm2s kernel(qs) robust orthog(l1.unem)" :. ivreg2 cinf (unem = l(1/3).unem), bw(3) gmm2s kernel(qs) robust orthog(l1.unem)}

{pstd}(Examples using Large N, Small T Panel Data)

{p 8 12 2}{stata "use http://fmwww.bc.edu/ec-p/data/macro/abdata.dta" : . use http://fmwww.bc.edu/ec-p/data/macro/abdata.dta }{p_end}

{p 8 12 2}{stata "tsset id year" :. tsset id year}

{pstd}(Autocorrelation-consistent inference in an IV regression)

{p 8 12 2}{stata "ivreg2 n (w k ys = d.w d.k d.ys d2.w d2.k d2.ys), bw(1) kernel(tru)": . ivreg2 n (w k ys = d.w d.k d.ys d2.w d2.k d2.ys), bw(1) kernel(tru)}

{pstd}(Two-step effic. GMM in the presence of arbitrary heteroskedasticity and autocorrelation)

{p 8 12 2}{stata "ivreg2 n (w k ys = d.w d.k d.ys d2.w d2.k d2.ys), bw(2) gmm2s kernel(tru) robust": . ivreg2 n (w k ys = d.w d.k d.ys d2.w d2.k d2.ys), bw(2) gmm2s kernel(tru) robust}

{pstd}(Two-step effic. GMM in the presence of arbitrary heterosked. and intragroup correlation)

{p 8 12 2}{stata "ivreg2 n (w k ys = d.w d.k d.ys d2.w d2.k d2.ys), gmm2s cluster(id)": . ivreg2 n (w k ys = d.w d.k d.ys d2.w d2.k d2.ys), gmm2s cluster(id)}


{marker s_refs}{...}
{title:References}

{p 4 8 2}Ahn, S. C. 1997. Orthogonality tests in linear models. 
{it:Oxford Bulletin of Economics and Statistics} 59: 183-186.

{p 4 8 2}Anderson, T. W. 1951. Estimating linear restrictions on regression coefficients
for multivariate normal distributions. {it:Annals of Mathematical Statistics} 22: 327-51.

{p 4 8 2}Anderson, T. W., and H. Rubin. 1949. Estimation of the parameters of a single equation 
in a complete system of stochastic equations. {it:Annals of Mathematical Statistics} 20:
46-63. 

{p 4 8 2}Anderson, T. W., and H. Rubin. 1950. The asymptotic properties of estimates of the parameters of a single 
equation in a complete system of stochastic equations. {it:Annals of Mathematical Statistics}
21: 570-582. 

{p 4 8 2}Baum, C. F., M. E. Schaffer, and S. Stillman. 2003. Instrumental variables and GMM:
Estimation and testing. {it:Stata Journal} 3: 1-31.

{p 4 8 2}Baum, C. F., M. E. Schaffer, and S. Stillman. 2007. Enhanced routines
for instrumental variables/generalized method of moments estimation and
testing. {it:Stata Journal} 7: 465-506.

{p 4 8 2}Breusch, T., H. Qian, P. Schmidt, and D. Wyhowski. 1999.
Redundancy of moment conditions.
{it:Journal of Econometrics} 9: 89-111.

{p 4 8 2}Chernozhukov, V., and C. Hansen. 2005. The reduced form:
A simple approach to inference with weak instruments.
Working paper, University of Chicago, Graduate School of Business.

{p 4 8 2}Cragg, J. G., and S. G. Donald. 1993. Testing identfiability and specification in
instrumental variables models. {it:Econometric Theory} 9: 222-240.

{p 4 8 2}Cushing, M. J., and M. G. McGarvey. 1999. Covariance matrix estimation.
In {it:Generalized Methods of Moments Estimation}, ed. L. Matyas. 
Cambridge: Cambridge University Press.

{p 4 8 2}Davidson, R., and J. G. MacKinnon. 1993.
{it:Estimation and Inference in Econometrics}. 2nd ed.
New York: Oxford University Press.

{p 4 8 2}Dufour, J.-M.  2003.  Identification, weak instruments and statistical inference
in econometrics. {it:Canadian Journal of Economics} 36: 767-808.

{p 4 8 2}Hall, A. R., G. D. Rudebusch, and D. W. Wilcox. 1996. Judging instrument relevance in
instrumental variables estimation.  {it:International Economic Review} 37: 283-298.

{p 4 8 2}Hansen, L. P., J. Heaton, and A. Yaron. 1996. Finite sample properties
of some alternative GMM estimators.  {it:Journal of Business and Economic Statistics} 14: 262-280.

{p 4 8 2}Hayashi, F. 2000. {it:Econometrics}. Princeton, NJ: Princeton
University Press.

{p 4 8 2}Kleibergen, F. 2007. Generalizing weak instrument robust statistics towards multiple parameters, unrestricted covariance matrices and identification statistics.
{it:Journal of Econometrics. Forthcoming.

{p 4 8 2}Kleibergen, F., and R. Paap. 2006. Generalized reduced rank tests using the singular value decomposition.
{it:Journal of Econometrics} 133: 97-126.

{p 4 8 2}Kleibergen, F., and M. E. Schaffer.  2007. ranktest: Stata module for testing the rank
of a matrix using the Kleibergen-Paap rk statistic.
{browse "http://ideas.repec.org/c/boc/bocode/s456865.html":http://ideas.repec.org/c/boc/bocode/s456865.html}

{p 4 8 2}Mikusheva, A., and B. P. Poi.  2006.
Tests and confidence sets with correct size when instruments are potentially weak. 
{it:Stata Journal} 6: 335-347.

{p 4 8 2}Moreira, M. J., and B. P. Poi.  2003.  Implementing tests with the correct size in the simultaneous equations model.  
{it:Stata Journal} 3: 57-70.

{p 4 8 2}Newey, W. K., and K. D. West. 1994. Automatic lag selection in covariance matrix estimation. 
{it:Review of Economic Studies} 61: 631-653.

{p 4 8 2}Shea, J. 1997.  Instrument relevance in multivariate linear models:
A simple measure.
{it:Review of Economics and Statistics} 49: 348-352.

{p 4 8 2}Stock, J. H., and J. H. Wright.  2000.  GMM with weak identification.
{it:Econometrica} 68: 1055-1096.

{p 4 8 2}Stock, J. H., and M. Yogo.  2005.  Testing for weak instruments in
linear IV regression.
In {it:Identification and Inference for Econometric Models:}
{it:Essays in Honor of Thomas Rothenberg},
ed. D. W. K. Andrews and J. H. Stock, 80-108.
Cambridge: Cambridge University Press.

{p 4 8 2}Wooldridge, J. M. 2002. {it:Econometric Analysis of Cross Section and Panel Data}. Cambridge, MA: MIT Press.


{marker s_acknow}{title:Acknowledgments}

{pstd}We thank various colleagues who helped us along the way, including
David Drukker,
Frank Kleibergen,
Austin Nichols,
Vince Wiggins,
and, not least, the users of {cmd:ivreg2}
who have provided suggestions,
spotted bugs,
and helped test the package.
We are also grateful to Jim Stock and Moto Yogo for permission to reproduce
their critical values for the Cragg-Donald statistic.


{title:Authors}

	Christopher F Baum, Boston College, USA
	baum@bc.edu

	Mark E Schaffer, Heriot-Watt University, UK
	m.e.schaffer@hw.ac.uk

	Steven Stillman, Motu Economic and Public Policy Research
	stillman@motu.org.nz


{marker s_citation}{title:Citation}

{pstd}{cmd:ivreg2} is not an official Stata command. It is a free contribution
to the research community, like a paper. Please cite it as such: {p_end}

{phang}Baum, C. F., M. E. Schaffer, and S. Stillman. 2007.
ivreg2: Stata module for extended instrumental variables/2SLS, GMM and
 AC/HAC, LIML, and k-class regression. Boston College Department of 
Economics, Statistical Software Components S425401. Downloadable from
{browse "http://ideas.repec.org/c/boc/bocode/s425401.html":http://ideas.repec.org/c/boc/bocode/s425401.html}.{p_end}


{title:Also see}

{psee}Manual:  {hi:[U] 23 Estimation and postestimation commands},{break}
               {hi:[U] 29 Overview of model estimation in Stata},{break}
	       {hi:[R] ivregress}

{psee}Online:  {helpb ivregress}, {helpb newey};
{helpb overid}, {helpb ivendog}, {helpb ivhettest}, {helpb ivreset},
{helpb xtivreg2}, {helpb xtoverid}, {helpb ranktest},
{helpb condivreg} (if installed);
{help est}, {help postest};
{helpb regress}{p_end}
